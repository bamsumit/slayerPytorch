
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <title>Optimizer &#8212; SLAYER PyTorch 0.1 documentation</title>
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/language_data.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Quantize module" href="quantizeParams.html" />
    <link rel="prev" title="Learning statistics" href="learningStats.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="module-slayerSNN.optimizer">
<span id="optimizer"></span><h1>Optimizer<a class="headerlink" href="#module-slayerSNN.optimizer" title="Permalink to this headline">¶</a></h1>
<dl class="py class">
<dt id="slayerSNN.optimizer.Nadam">
<em class="property">class </em><code class="sig-prename descclassname">slayerSNN.optimizer.</code><code class="sig-name descname">Nadam</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">params</span></em>, <em class="sig-param"><span class="n">lr</span><span class="o">=</span><span class="default_value">0.001</span></em>, <em class="sig-param"><span class="n">betas</span><span class="o">=</span><span class="default_value">0.9, 0.999</span></em>, <em class="sig-param"><span class="n">eps</span><span class="o">=</span><span class="default_value">1e-08</span></em>, <em class="sig-param"><span class="n">weight_decay</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">amsgrad</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/slayerSNN/optimizer.html#Nadam"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#slayerSNN.optimizer.Nadam" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements Nadam algorithm. (Modified Adam from <a class="reference external" href="https://pytorch.org/docs/stable/_modules/torch/optim/adam.html#Adam">PyTorch</a>)</p>
<p>It has been proposed in <a class="reference external" href="https://openreview.net/pdf?id=OM0jvwB8jIp57ZJjtNEZ">Incorporating Nesterov Momentum into Adam</a>.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">params</span></code> (iterable): iterable of parameters to optimize or dicts defining parameter groups.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">lr</span></code> (<code class="docutils literal notranslate"><span class="pre">float</span></code>, optional): learning rate (default: 1e-3).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">betas</span></code> (Tuple[<code class="docutils literal notranslate"><span class="pre">float</span></code>, <code class="docutils literal notranslate"><span class="pre">float</span></code>], optional): coefficients used for computing
running averages of gradient and its square (default: (0.9, 0.999)).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">eps</span></code> (<code class="docutils literal notranslate"><span class="pre">float</span></code>, optional): term added to the denominator to improve
numerical stability (default: 1e-8).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">weight_decay</span></code> (<code class="docutils literal notranslate"><span class="pre">float</span></code>, optional): weight decay (L2 penalty) (default: 0).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">amsgrad</span></code> (<code class="docutils literal notranslate"><span class="pre">boolean</span></code>, optional): whether to use the AMSGrad variant of this
algorithm from the paper <a class="reference external" href="https://openreview.net/forum?id=ryQu7f-RZ">On the Convergence of Adam and Beyond</a>
(default: False).</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="slayerSNN.optimizer.Nadam.step">
<code class="sig-name descname">step</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">closure</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/slayerSNN/optimizer.html#Nadam.step"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#slayerSNN.optimizer.Nadam.step" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs a single optimization step.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><ul class="simple">
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">closure</span></code> (callable, optional): A closure that reevaluates the model</dt><dd><p>and returns the loss.</p>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">SLAYER PyTorch</a></h1>








<h3>Navigation</h3>
<p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="slayerSNN.html">SLAYER PyTorch main</a></li>
<li class="toctree-l1"><a class="reference internal" href="slayer.html">SLAYER module</a></li>
<li class="toctree-l1"><a class="reference internal" href="slayerLoihi.html">SLAYER Loihi module</a></li>
<li class="toctree-l1"><a class="reference internal" href="slayerParams.html">SLAYER Parameter</a></li>
<li class="toctree-l1"><a class="reference internal" href="spikeClassifier.html">Spike Classifier</a></li>
<li class="toctree-l1"><a class="reference internal" href="spikeLoss.html">Spike Loss</a></li>
<li class="toctree-l1"><a class="reference internal" href="spikeIO.html">Spike Input/Output</a></li>
<li class="toctree-l1"><a class="reference internal" href="learningStats.html">Learning statistics</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="quantizeParams.html">Quantize module</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="learningStats.html" title="previous chapter">Learning statistics</a></li>
      <li>Next: <a href="quantizeParams.html" title="next chapter">Quantize module</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2019, Sumit Bam Shrestha.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 3.0.3</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="_sources/optimizer.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>